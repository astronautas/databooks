{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"databooks databooks is a package to ease the collaboration between data scientists using Jupyter notebooks , by reducing the number of git conflicts between different notebooks and resolution of git conflicts when encountered. The key features include: CLI tool Clear notebook metadata Resolve git conflicts Simple to use Simple API for using modelling and comparing notebooks using Pydantic Requirements databooks is built on top of: Python 3.8+ Typer Rich Pydantic GitPython Installation pip install databooks Usage Clear metadata Simply specify the paths for notebook files to remove metadata. By doing so, we can already avoid many of the conflicts. $ databooks meta [OPTIONS] PATHS... Fix git conflicts for notebooks Specify the paths for notebook files with conflicts to be fixed. Then, databooks finds the source notebooks that caused the conflicts and compares them (so no JSON manipulation!) $ databooks fix [OPTIONS] PATHS... License This project is licensed under the terms of the MIT license.","title":"Overview"},{"location":"#databooks","text":"databooks is a package to ease the collaboration between data scientists using Jupyter notebooks , by reducing the number of git conflicts between different notebooks and resolution of git conflicts when encountered. The key features include: CLI tool Clear notebook metadata Resolve git conflicts Simple to use Simple API for using modelling and comparing notebooks using Pydantic","title":"databooks"},{"location":"#requirements","text":"databooks is built on top of: Python 3.8+ Typer Rich Pydantic GitPython","title":"Requirements"},{"location":"#installation","text":"pip install databooks","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#clear-metadata","text":"Simply specify the paths for notebook files to remove metadata. By doing so, we can already avoid many of the conflicts. $ databooks meta [OPTIONS] PATHS...","title":"Clear metadata"},{"location":"#fix-git-conflicts-for-notebooks","text":"Specify the paths for notebook files with conflicts to be fixed. Then, databooks finds the source notebooks that caused the conflicts and compares them (so no JSON manipulation!) $ databooks fix [OPTIONS] PATHS...","title":"Fix git conflicts for notebooks"},{"location":"#license","text":"This project is licensed under the terms of the MIT license.","title":"License"},{"location":"API/","text":"Overview Though databooks was built to be used as a CLI application, you could also use the databooks data models to manipulate notebooks in other scripts. All the data models are based on Pydantic models . That means that we have a convenient way to read/write notebooks (JSON files) and ensure that the JSON is actually a valid notebook. By using databooks to parse your notebooks, you can also count on IDE support. Using databooks in python scripts is as simple as from databooks import JupyterNotebook notebook = JupyterNotebook.parse_file(path=\"path/to/your/notebook\") assert list(dict(notebook).keys()) == ['nbformat', 'nbformat_minor', 'metadata', 'cells'] For more information on the internal workings and what is implemented, see how it works .","title":"Overview"},{"location":"API/#overview","text":"Though databooks was built to be used as a CLI application, you could also use the databooks data models to manipulate notebooks in other scripts. All the data models are based on Pydantic models . That means that we have a convenient way to read/write notebooks (JSON files) and ensure that the JSON is actually a valid notebook. By using databooks to parse your notebooks, you can also count on IDE support. Using databooks in python scripts is as simple as from databooks import JupyterNotebook notebook = JupyterNotebook.parse_file(path=\"path/to/your/notebook\") assert list(dict(notebook).keys()) == ['nbformat', 'nbformat_minor', 'metadata', 'cells'] For more information on the internal workings and what is implemented, see how it works .","title":"Overview"},{"location":"CLI/","text":"databooks Databooks - set of helpers to ease collaboration of data scientists using Jupyter Notebooks. Easily resolve git conflicts and remove metadata to reduce the number of conflicts. Usage : $ databooks [OPTIONS] COMMAND [ARGS]... Options : --version --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : diff : Show differences between notebooks (not... fix : Fix git conflicts for notebooks by getting... meta : Clear both notebook and cell metadata databooks diff Show differences between notebooks (not implemented) Usage : $ databooks diff [OPTIONS] Options : --help : Show this message and exit. databooks fix Fix git conflicts for notebooks by getting unmerged blobs from git index comparing them and returning a valid notebook with the differences - see git docs Usage : $ databooks fix [OPTIONS] PATHS... Arguments : PATHS... : Path(s) of notebook files with conflicts [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --metadata-first / --no-metadata-first : Whether or not to keep the metadata from the first/current notebook [default: True] --cells-first / --no-cells-first : Whether to keep the cells from the first or last notebook. Omit to keep both -i, --interactive : Interactively resolve the conflicts (not implemented) [default: False] --verbose / --no-verbose : Log processed files in console [default: False] --help : Show this message and exit. databooks meta Clear both notebook and cell metadata Usage : $ databooks meta [OPTIONS] PATHS... Arguments : PATHS... : Path(s) of notebook files [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --prefix TEXT : Prefix to add to filepath when writing files [default: ] --suffix TEXT : Suffix to add to filepath when writing files [default: ] --rm-outs / --no-rm-outs : Whether to remove cell outputs [default: False] --rm-exec / --no-rm-exec : Whether to remove the cell execution counts [default: True] --nb-meta-keep TEXT : Notebook metadata fields to keep [default: ] --cell-meta-keep TEXT : Cells metadata fields to keep [default: ] -w, --overwrite : Confirm overwrite of files [default: False] --check : Don't write files but check whether there is unwanted metadata [default: False] -v, --verbose : Log processed files in console [default: False] --help : Show this message and exit.","title":"CLI"},{"location":"CLI/#databooks","text":"Databooks - set of helpers to ease collaboration of data scientists using Jupyter Notebooks. Easily resolve git conflicts and remove metadata to reduce the number of conflicts. Usage : $ databooks [OPTIONS] COMMAND [ARGS]... Options : --version --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : diff : Show differences between notebooks (not... fix : Fix git conflicts for notebooks by getting... meta : Clear both notebook and cell metadata","title":"databooks"},{"location":"CLI/#databooks-diff","text":"Show differences between notebooks (not implemented) Usage : $ databooks diff [OPTIONS] Options : --help : Show this message and exit.","title":"databooks diff"},{"location":"CLI/#databooks-fix","text":"Fix git conflicts for notebooks by getting unmerged blobs from git index comparing them and returning a valid notebook with the differences - see git docs Usage : $ databooks fix [OPTIONS] PATHS... Arguments : PATHS... : Path(s) of notebook files with conflicts [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --metadata-first / --no-metadata-first : Whether or not to keep the metadata from the first/current notebook [default: True] --cells-first / --no-cells-first : Whether to keep the cells from the first or last notebook. Omit to keep both -i, --interactive : Interactively resolve the conflicts (not implemented) [default: False] --verbose / --no-verbose : Log processed files in console [default: False] --help : Show this message and exit.","title":"databooks fix"},{"location":"CLI/#databooks-meta","text":"Clear both notebook and cell metadata Usage : $ databooks meta [OPTIONS] PATHS... Arguments : PATHS... : Path(s) of notebook files [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --prefix TEXT : Prefix to add to filepath when writing files [default: ] --suffix TEXT : Suffix to add to filepath when writing files [default: ] --rm-outs / --no-rm-outs : Whether to remove cell outputs [default: False] --rm-exec / --no-rm-exec : Whether to remove the cell execution counts [default: True] --nb-meta-keep TEXT : Notebook metadata fields to keep [default: ] --cell-meta-keep TEXT : Cells metadata fields to keep [default: ] -w, --overwrite : Confirm overwrite of files [default: False] --check : Don't write files but check whether there is unwanted metadata [default: False] -v, --verbose : Log processed files in console [default: False] --help : Show this message and exit.","title":"databooks meta"},{"location":"alternatives/","text":"Alternatives and comparisons There are many tools to improve the development experience with notebooks. databooks also takes inspiration from other existing packages. nb-clean nb-clean provides a CLI tool to clear notebook metadata from Jupyter notebooks. Differences: nb-clean does not offer a way to resolve conflicts. nbdime nbdime stands for \"notebook diff and merge\". It offers a way to compare and display the differences in the terminal. It offers a way to gracefully merge notebooks. Differences: nb-dime does not offer a way to remove metadata. databooks also fixes git merge conflicts instead of offering a way to do git merges (\"ask for forgiveness not permission\"). nbdev nbdev offers a way to use notebooks to develop python packages, converting notebooks to python scripts and documentation. Also offers installation of pre-commit hooks to remove notebook metadata and a way to resolve git conflicts. Differences: nbdev is closer to an opinionated template for developing packages with notebooks. databooks is a configurable CLI tool for metadata removal and conflict resolution.","title":"Alternatives and comparisons"},{"location":"alternatives/#alternatives-and-comparisons","text":"There are many tools to improve the development experience with notebooks. databooks also takes inspiration from other existing packages.","title":"Alternatives and comparisons"},{"location":"alternatives/#nb-clean","text":"nb-clean provides a CLI tool to clear notebook metadata from Jupyter notebooks. Differences: nb-clean does not offer a way to resolve conflicts.","title":"nb-clean"},{"location":"alternatives/#nbdime","text":"nbdime stands for \"notebook diff and merge\". It offers a way to compare and display the differences in the terminal. It offers a way to gracefully merge notebooks. Differences: nb-dime does not offer a way to remove metadata. databooks also fixes git merge conflicts instead of offering a way to do git merges (\"ask for forgiveness not permission\").","title":"nbdime"},{"location":"alternatives/#nbdev","text":"nbdev offers a way to use notebooks to develop python packages, converting notebooks to python scripts and documentation. Also offers installation of pre-commit hooks to remove notebook metadata and a way to resolve git conflicts. Differences: nbdev is closer to an opinionated template for developing packages with notebooks. databooks is a configurable CLI tool for metadata removal and conflict resolution.","title":"nbdev"},{"location":"common/","text":"Common set of miscellaneous functions. expand_paths ( paths , ignore ) Get paths of existing file from list of directory or file paths. Parameters: Name Type Description Default paths List[pathlib.Path] Paths to consider (can be directories or files) required ignore List[str] Glob expressions of files to ignore required Returns: Type Description List[pathlib.Path] List of existing paths for notebooks Source code in databooks/common.py def expand_paths ( paths : List [ Path ], ignore : List [ str ]) -> List [ Path ]: \"\"\" Get paths of existing file from list of directory or file paths. :param paths: Paths to consider (can be directories or files) :param ignore: Glob expressions of files to ignore :return: List of existing paths for notebooks \"\"\" paths = list ( chain . from_iterable ( list ( path . rglob ( \"*.ipynb\" )) if path . is_dir () else [ path ] for path in paths ) ) return [ p for p in paths if not any ( p . match ( i ) for i in ignore ) and p . exists () and p . suffix == \".ipynb\" ] write_notebook ( nb , path ) Write notebook to a path. Source code in databooks/common.py def write_notebook ( nb : JupyterNotebook , path : Path ) -> None : \"\"\"Write notebook to a path.\"\"\" with path . open ( \"w\" ) as f : json . dump ( nb . dict (), fp = f , indent = 2 )","title":"Common utils"},{"location":"common/#databooks.common.expand_paths","text":"Get paths of existing file from list of directory or file paths. Parameters: Name Type Description Default paths List[pathlib.Path] Paths to consider (can be directories or files) required ignore List[str] Glob expressions of files to ignore required Returns: Type Description List[pathlib.Path] List of existing paths for notebooks Source code in databooks/common.py def expand_paths ( paths : List [ Path ], ignore : List [ str ]) -> List [ Path ]: \"\"\" Get paths of existing file from list of directory or file paths. :param paths: Paths to consider (can be directories or files) :param ignore: Glob expressions of files to ignore :return: List of existing paths for notebooks \"\"\" paths = list ( chain . from_iterable ( list ( path . rglob ( \"*.ipynb\" )) if path . is_dir () else [ path ] for path in paths ) ) return [ p for p in paths if not any ( p . match ( i ) for i in ignore ) and p . exists () and p . suffix == \".ipynb\" ]","title":"expand_paths()"},{"location":"common/#databooks.common.write_notebook","text":"Write notebook to a path. Source code in databooks/common.py def write_notebook ( nb : JupyterNotebook , path : Path ) -> None : \"\"\"Write notebook to a path.\"\"\" with path . open ( \"w\" ) as f : json . dump ( nb . dict (), fp = f , indent = 2 )","title":"write_notebook()"},{"location":"conflicts/","text":"Functions to resolve any git conflicts between notebooks. cells_equals ( diff_cells ) Return if cells in DiffCells are equal. Source code in databooks/conflicts.py def cells_equals ( diff_cells : Cells [ Tuple [ List [ Cell ], List [ Cell ]]]) -> List [ bool ]: \"\"\"Return if cells in `DiffCells` are equal.\"\"\" return [ cell_first == cell_last for cell_first , cell_last in diff_cells ] conflict2nb ( conflict_file , * , meta_first = True , cells_first = None , cell_fields_ignore = ( 'id' , 'execution_count' ), ignore_none = True , verbose = False ) Merge diffs from conflicts and return valid a notebook. Parameters: Name Type Description Default conflict_file ConflictFile A databooks.git_utils.ConflictFile with conflicts required meta_first bool Whether to keep the metadata of the first or last notebook True cells_first Optional[bool] Whether to keep the cells of the first or last notebook None ignore_none bool Keep all metadata fields even if it's included in only one notebook True cell_fields_ignore Sequence[str] Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered ('id', 'execution_count') verbose bool Log written files and metadata conflicts False Returns: Type Description JupyterNotebook Resolved conflicts as a databooks.data_models.notebook.JupyterNotebook model Source code in databooks/conflicts.py def conflict2nb ( conflict_file : ConflictFile , * , meta_first : bool = True , cells_first : Optional [ bool ] = None , cell_fields_ignore : Sequence [ str ] = ( \"id\" , \"execution_count\" ), ignore_none : bool = True , verbose : bool = False , ) -> JupyterNotebook : \"\"\" Merge diffs from conflicts and return valid a notebook. :param conflict_file: A `databooks.git_utils.ConflictFile` with conflicts :param meta_first: Whether to keep the metadata of the first or last notebook :param cells_first: Whether to keep the cells of the first or last notebook :param ignore_none: Keep all metadata fields even if it's included in only one notebook :param cell_fields_ignore: Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered :param verbose: Log written files and metadata conflicts :return: Resolved conflicts as a `databooks.data_models.notebook.JupyterNotebook` model \"\"\" if verbose : set_verbose ( logger ) nb_1 = JupyterNotebook . parse_raw ( conflict_file . first_contents ) nb_2 = JupyterNotebook . parse_raw ( conflict_file . last_contents ) if nb_1 . metadata != nb_2 . metadata : msg = ( f \"Notebook metadata conflict for { conflict_file . filename } . Keeping \" + \"first.\" if meta_first else \"last.\" ) logger . debug ( msg ) diff_nb = nb_1 - nb_2 nb = diff_nb . resolve ( ignore_none = ignore_none , keep_first = meta_first , keep_first_cells = cells_first , first_id = conflict_file . first_log , last_id = conflict_file . last_log , ) if not isinstance ( nb , JupyterNotebook ): raise RuntimeError ( f \"Expected `databooks.JupyterNotebook`, got { type ( nb ) } .\" ) logger . debug ( f \"Resolved conflicts in { conflict_file . filename } .\" ) return nb conflicts2nbs ( conflict_files , * , progress_callback =< function < lambda > at 0x7efc4546a820 > , ** conflict2nb_kwargs ) Get notebooks from conflicts. Wrap databooks.conflicts.conflict2nb to write notebooks to list of databooks.git_utils.ConflictFile . Parameters: Name Type Description Default conflict_files List[ConflictFile] Files with source conflict files and one-liner git logs required progress_callback Callable[[], None] Callback function to report progress <function <lambda> at 0x7efc4546a820> conflict2nb_kwargs Any Keyword arguments to be passed to databooks.conflicts.conflict2nb {} Returns: Type Description None Source code in databooks/conflicts.py def conflicts2nbs ( conflict_files : List [ ConflictFile ], * , progress_callback : Callable [[], None ] = lambda : None , ** conflict2nb_kwargs : Any , ) -> None : \"\"\" Get notebooks from conflicts. Wrap `databooks.conflicts.conflict2nb` to write notebooks to list of `databooks.git_utils.ConflictFile`. :param conflict_files: Files with source conflict files and one-liner git logs :param progress_callback: Callback function to report progress :param conflict2nb_kwargs: Keyword arguments to be passed to `databooks.conflicts.conflict2nb` :return: \"\"\" for conflict in conflict_files : nb = conflict2nb ( conflict , ** conflict2nb_kwargs ) write_notebook ( nb = nb , path = conflict . filename ) progress_callback () path2conflicts ( nb_paths , repo = None ) Get the difference model from the path based on the git conflict information. Parameters: Name Type Description Default nb_paths List[Path] Path to file with conflicts (must be notebook paths) required repo Optional[Repo] The git repo to look for conflicts None Returns: Type Description List[ConflictFile] Generator of DiffModel s, to be resolved Source code in databooks/conflicts.py def path2conflicts ( nb_paths : List [ Path ], repo : Optional [ Repo ] = None ) -> List [ ConflictFile ]: \"\"\" Get the difference model from the path based on the git conflict information. :param nb_paths: Path to file with conflicts (must be notebook paths) :param repo: The git repo to look for conflicts :return: Generator of `DiffModel`s, to be resolved \"\"\" if any ( nb_path . suffix not in ( \"\" , \".ipynb\" ) for nb_path in nb_paths ): raise ValueError ( \"Expected either notebook files, a directory or glob expression.\" ) common_parent = max ( set . intersection ( * [ set ( p . parents ) for p in nb_paths ])) repo = get_repo ( common_parent ) if repo is None else repo return [ file for file in get_conflict_blobs ( repo = repo ) if any ( file . filename . match ( str ( p . name )) for p in nb_paths ) ]","title":"Conflicts"},{"location":"conflicts/#databooks.conflicts.cells_equals","text":"Return if cells in DiffCells are equal. Source code in databooks/conflicts.py def cells_equals ( diff_cells : Cells [ Tuple [ List [ Cell ], List [ Cell ]]]) -> List [ bool ]: \"\"\"Return if cells in `DiffCells` are equal.\"\"\" return [ cell_first == cell_last for cell_first , cell_last in diff_cells ]","title":"cells_equals()"},{"location":"conflicts/#databooks.conflicts.conflict2nb","text":"Merge diffs from conflicts and return valid a notebook. Parameters: Name Type Description Default conflict_file ConflictFile A databooks.git_utils.ConflictFile with conflicts required meta_first bool Whether to keep the metadata of the first or last notebook True cells_first Optional[bool] Whether to keep the cells of the first or last notebook None ignore_none bool Keep all metadata fields even if it's included in only one notebook True cell_fields_ignore Sequence[str] Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered ('id', 'execution_count') verbose bool Log written files and metadata conflicts False Returns: Type Description JupyterNotebook Resolved conflicts as a databooks.data_models.notebook.JupyterNotebook model Source code in databooks/conflicts.py def conflict2nb ( conflict_file : ConflictFile , * , meta_first : bool = True , cells_first : Optional [ bool ] = None , cell_fields_ignore : Sequence [ str ] = ( \"id\" , \"execution_count\" ), ignore_none : bool = True , verbose : bool = False , ) -> JupyterNotebook : \"\"\" Merge diffs from conflicts and return valid a notebook. :param conflict_file: A `databooks.git_utils.ConflictFile` with conflicts :param meta_first: Whether to keep the metadata of the first or last notebook :param cells_first: Whether to keep the cells of the first or last notebook :param ignore_none: Keep all metadata fields even if it's included in only one notebook :param cell_fields_ignore: Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered :param verbose: Log written files and metadata conflicts :return: Resolved conflicts as a `databooks.data_models.notebook.JupyterNotebook` model \"\"\" if verbose : set_verbose ( logger ) nb_1 = JupyterNotebook . parse_raw ( conflict_file . first_contents ) nb_2 = JupyterNotebook . parse_raw ( conflict_file . last_contents ) if nb_1 . metadata != nb_2 . metadata : msg = ( f \"Notebook metadata conflict for { conflict_file . filename } . Keeping \" + \"first.\" if meta_first else \"last.\" ) logger . debug ( msg ) diff_nb = nb_1 - nb_2 nb = diff_nb . resolve ( ignore_none = ignore_none , keep_first = meta_first , keep_first_cells = cells_first , first_id = conflict_file . first_log , last_id = conflict_file . last_log , ) if not isinstance ( nb , JupyterNotebook ): raise RuntimeError ( f \"Expected `databooks.JupyterNotebook`, got { type ( nb ) } .\" ) logger . debug ( f \"Resolved conflicts in { conflict_file . filename } .\" ) return nb","title":"conflict2nb()"},{"location":"conflicts/#databooks.conflicts.conflicts2nbs","text":"Get notebooks from conflicts. Wrap databooks.conflicts.conflict2nb to write notebooks to list of databooks.git_utils.ConflictFile . Parameters: Name Type Description Default conflict_files List[ConflictFile] Files with source conflict files and one-liner git logs required progress_callback Callable[[], None] Callback function to report progress <function <lambda> at 0x7efc4546a820> conflict2nb_kwargs Any Keyword arguments to be passed to databooks.conflicts.conflict2nb {} Returns: Type Description None Source code in databooks/conflicts.py def conflicts2nbs ( conflict_files : List [ ConflictFile ], * , progress_callback : Callable [[], None ] = lambda : None , ** conflict2nb_kwargs : Any , ) -> None : \"\"\" Get notebooks from conflicts. Wrap `databooks.conflicts.conflict2nb` to write notebooks to list of `databooks.git_utils.ConflictFile`. :param conflict_files: Files with source conflict files and one-liner git logs :param progress_callback: Callback function to report progress :param conflict2nb_kwargs: Keyword arguments to be passed to `databooks.conflicts.conflict2nb` :return: \"\"\" for conflict in conflict_files : nb = conflict2nb ( conflict , ** conflict2nb_kwargs ) write_notebook ( nb = nb , path = conflict . filename ) progress_callback ()","title":"conflicts2nbs()"},{"location":"conflicts/#databooks.conflicts.path2conflicts","text":"Get the difference model from the path based on the git conflict information. Parameters: Name Type Description Default nb_paths List[Path] Path to file with conflicts (must be notebook paths) required repo Optional[Repo] The git repo to look for conflicts None Returns: Type Description List[ConflictFile] Generator of DiffModel s, to be resolved Source code in databooks/conflicts.py def path2conflicts ( nb_paths : List [ Path ], repo : Optional [ Repo ] = None ) -> List [ ConflictFile ]: \"\"\" Get the difference model from the path based on the git conflict information. :param nb_paths: Path to file with conflicts (must be notebook paths) :param repo: The git repo to look for conflicts :return: Generator of `DiffModel`s, to be resolved \"\"\" if any ( nb_path . suffix not in ( \"\" , \".ipynb\" ) for nb_path in nb_paths ): raise ValueError ( \"Expected either notebook files, a directory or glob expression.\" ) common_parent = max ( set . intersection ( * [ set ( p . parents ) for p in nb_paths ])) repo = get_repo ( common_parent ) if repo is None else repo return [ file for file in get_conflict_blobs ( repo = repo ) if any ( file . filename . match ( str ( p . name )) for p in nb_paths ) ]","title":"path2conflicts()"},{"location":"contrib/","text":"Contributing We are always looking for contributions! You can find below some relevant information and standards for databooks . Setup \u2699\ufe0f After cloning the repo , make sure to set up the environment. Poetry \ud83d\udcdc We use Poetry for both managing environments and packaging. That means you need to install poetry but from there you can use the tool to create the environment. pip install poetry==1.1.12 poetry install # installs prod and dev dependencies Usage Remember that to use the environment you can use the poetry run <COMMAND> command or initialize the shell with poetry shell . For example, if you want to create the coverage report you could run poetry run pytest --cov=databooks tests/ or alternatively poetry shell pytest --cov=databooks tests/ Development \ud83d\udee0 We welcome new features, bugfixes or enhancements (whether on code or docs). There are a few standards we adhere to, that are required for new features. Mypy We use type hints! Not only that, they are enforced and checked (with Mypy ). This is actually the reason for supporting Python 3.8+. There are a couple of reasons for using type hints, mainly: Better code coverage (avoid errors during runtime) Improve code understanding As databooks uses both Typer and Pydantic , types are not only for developer hints, but they are also used to cast notebook (JSON) values to the correct types as well as user inputs in the CLI If you are not familiar with type hints and Mypy, a good starting point is watching the Type-checked Python in the real world - PyCon 2018 talk. Linting In regards to code quality, we use a couple of linting tools to maintain the same \"style\" and uphold to the same standards. For that, we use: Black for code formatting isort for imports Flake8 for style enforcement Docs \ud83d\udcda As for documentation, the databooks documentation \"lives\" both on the code itself and supporting documentation (markdown) files. Code Code docs include annotating type hints as well as function docstrings. For that, we use the reStructuredText -like format. Providing docstrings not only give a clear way to document the code, but it is also picked up by MkDocs . MkDocs MkDocs gives a simple way to write markdown files that get rendered as HTML (under a certain theme) and served as documentation. We use MkDocs with different extensions. We use mkdocstrings to link function docstrings with the existing documentation. You can check the generation of documentation by running from the project root mkdocs serve mike We also use the mike plugin in MkDocs to publish and keep different versions of documentation. Tests \ud83d\uddf3 We use unit tests to ensure that our package works as expected. We use pytest for testing and Pytest-cov for checking how much of the code is covered in our tests. The tests should mimic the package directory structure. The tests are also written to serve as an example of how to use the classes and methods and expected outputs. The coverage is also added to the documentation. For that we use MkDocs Coverage Plugin . For that we need a htmlcov/ directory that is generated by Pytest-cov by running from the project root pytest --cov-report html --cov=databooks tests/ Publishing Publishing is automatically done via Github Actions to PiPy. After published, a new tag and release are created. A new docs version is also published if all previous steps are successful. Contributors \ud83d\udc68\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb databooks was created by Murilo Cunha , and is maintained by dataroots . Acknowledgements Special thanks to: Bart and Nick for feedback and support.","title":"Contributing"},{"location":"contrib/#contributing","text":"We are always looking for contributions! You can find below some relevant information and standards for databooks .","title":"Contributing"},{"location":"contrib/#setup","text":"After cloning the repo , make sure to set up the environment.","title":"Setup \u2699\ufe0f"},{"location":"contrib/#poetry","text":"We use Poetry for both managing environments and packaging. That means you need to install poetry but from there you can use the tool to create the environment. pip install poetry==1.1.12 poetry install # installs prod and dev dependencies","title":"Poetry \ud83d\udcdc"},{"location":"contrib/#usage","text":"Remember that to use the environment you can use the poetry run <COMMAND> command or initialize the shell with poetry shell . For example, if you want to create the coverage report you could run poetry run pytest --cov=databooks tests/ or alternatively poetry shell pytest --cov=databooks tests/","title":"Usage"},{"location":"contrib/#development","text":"We welcome new features, bugfixes or enhancements (whether on code or docs). There are a few standards we adhere to, that are required for new features.","title":"Development \ud83d\udee0"},{"location":"contrib/#mypy","text":"We use type hints! Not only that, they are enforced and checked (with Mypy ). This is actually the reason for supporting Python 3.8+. There are a couple of reasons for using type hints, mainly: Better code coverage (avoid errors during runtime) Improve code understanding As databooks uses both Typer and Pydantic , types are not only for developer hints, but they are also used to cast notebook (JSON) values to the correct types as well as user inputs in the CLI If you are not familiar with type hints and Mypy, a good starting point is watching the Type-checked Python in the real world - PyCon 2018 talk.","title":"Mypy"},{"location":"contrib/#linting","text":"In regards to code quality, we use a couple of linting tools to maintain the same \"style\" and uphold to the same standards. For that, we use: Black for code formatting isort for imports Flake8 for style enforcement","title":"Linting"},{"location":"contrib/#docs","text":"As for documentation, the databooks documentation \"lives\" both on the code itself and supporting documentation (markdown) files.","title":"Docs \ud83d\udcda"},{"location":"contrib/#code","text":"Code docs include annotating type hints as well as function docstrings. For that, we use the reStructuredText -like format. Providing docstrings not only give a clear way to document the code, but it is also picked up by MkDocs .","title":"Code"},{"location":"contrib/#mkdocs","text":"MkDocs gives a simple way to write markdown files that get rendered as HTML (under a certain theme) and served as documentation. We use MkDocs with different extensions. We use mkdocstrings to link function docstrings with the existing documentation. You can check the generation of documentation by running from the project root mkdocs serve","title":"MkDocs"},{"location":"contrib/#mike","text":"We also use the mike plugin in MkDocs to publish and keep different versions of documentation.","title":"mike"},{"location":"contrib/#tests","text":"We use unit tests to ensure that our package works as expected. We use pytest for testing and Pytest-cov for checking how much of the code is covered in our tests. The tests should mimic the package directory structure. The tests are also written to serve as an example of how to use the classes and methods and expected outputs. The coverage is also added to the documentation. For that we use MkDocs Coverage Plugin . For that we need a htmlcov/ directory that is generated by Pytest-cov by running from the project root pytest --cov-report html --cov=databooks tests/","title":"Tests \ud83d\uddf3"},{"location":"contrib/#publishing","text":"Publishing is automatically done via Github Actions to PiPy. After published, a new tag and release are created. A new docs version is also published if all previous steps are successful.","title":"Publishing"},{"location":"contrib/#contributors","text":"databooks was created by Murilo Cunha , and is maintained by dataroots .","title":"Contributors \ud83d\udc68\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb"},{"location":"contrib/#acknowledgements","text":"Special thanks to: Bart and Nick for feedback and support.","title":"Acknowledgements"},{"location":"git_utils/","text":"Git helper functions. ConflictFile dataclass Container for path and different versions of conflicted notebooks. UnmergedBlob dataclass Container for git unmerged blobs. blob2commit ( blob , repo ) Get the short commit message from blob hash. Source code in databooks/git_utils.py def blob2commit ( blob : Blob , repo : Repo ) -> str : \"\"\"Get the short commit message from blob hash.\"\"\" _git = Git ( working_dir = repo . working_dir ) commit_id = _git . log ( find_object = blob , max_count = 1 , all = True , oneline = True ) return ( commit_id if len ( commit_id ) > 0 else _git . stash ( \"list\" , \"--oneline\" , \"--max-count\" , \"1\" , \"--find-object\" , blob ) ) get_conflict_blobs ( repo ) Get the source files for conflicts. Source code in databooks/git_utils.py def get_conflict_blobs ( repo : Repo ) -> List [ ConflictFile ]: \"\"\"Get the source files for conflicts.\"\"\" unmerged_blobs = repo . index . unmerged_blobs () blobs = ( UnmergedBlob ( filename = Path ( k ), stage = dict ( v )) for k , v in unmerged_blobs . items () if 0 not in dict ( v ) . keys () # only get blobs that could not be merged ) if not isinstance ( repo . working_dir , ( Path , str )): raise RuntimeError ( \"Expected `repo` to be `pathlib.Path` or `str`, got\" f \" { type ( repo . working_dir ) } .\" ) return [ ConflictFile ( filename = repo . working_dir / blob . filename , first_log = blob2commit ( blob = blob . stage [ 2 ], repo = repo ), last_log = blob2commit ( blob = blob . stage [ 3 ], repo = repo ), first_contents = repo . git . show ( blob . stage [ 2 ]), last_contents = repo . git . show ( blob . stage [ 3 ]), ) for blob in blobs ] get_repo ( path = PosixPath ( '/home/runner/work/databooks/databooks' )) Find git repo in current or parent directories. Source code in databooks/git_utils.py def get_repo ( path : Path = Path . cwd ()) -> Repo : \"\"\"Find git repo in current or parent directories.\"\"\" repo = Repo ( path = path , search_parent_directories = True ) logger . info ( f \"Repo found at: { repo . working_dir } \" ) return repo","title":"Git"},{"location":"git_utils/#databooks.git_utils.ConflictFile","text":"Container for path and different versions of conflicted notebooks.","title":"ConflictFile"},{"location":"git_utils/#databooks.git_utils.UnmergedBlob","text":"Container for git unmerged blobs.","title":"UnmergedBlob"},{"location":"git_utils/#databooks.git_utils.blob2commit","text":"Get the short commit message from blob hash. Source code in databooks/git_utils.py def blob2commit ( blob : Blob , repo : Repo ) -> str : \"\"\"Get the short commit message from blob hash.\"\"\" _git = Git ( working_dir = repo . working_dir ) commit_id = _git . log ( find_object = blob , max_count = 1 , all = True , oneline = True ) return ( commit_id if len ( commit_id ) > 0 else _git . stash ( \"list\" , \"--oneline\" , \"--max-count\" , \"1\" , \"--find-object\" , blob ) )","title":"blob2commit()"},{"location":"git_utils/#databooks.git_utils.get_conflict_blobs","text":"Get the source files for conflicts. Source code in databooks/git_utils.py def get_conflict_blobs ( repo : Repo ) -> List [ ConflictFile ]: \"\"\"Get the source files for conflicts.\"\"\" unmerged_blobs = repo . index . unmerged_blobs () blobs = ( UnmergedBlob ( filename = Path ( k ), stage = dict ( v )) for k , v in unmerged_blobs . items () if 0 not in dict ( v ) . keys () # only get blobs that could not be merged ) if not isinstance ( repo . working_dir , ( Path , str )): raise RuntimeError ( \"Expected `repo` to be `pathlib.Path` or `str`, got\" f \" { type ( repo . working_dir ) } .\" ) return [ ConflictFile ( filename = repo . working_dir / blob . filename , first_log = blob2commit ( blob = blob . stage [ 2 ], repo = repo ), last_log = blob2commit ( blob = blob . stage [ 3 ], repo = repo ), first_contents = repo . git . show ( blob . stage [ 2 ]), last_contents = repo . git . show ( blob . stage [ 3 ]), ) for blob in blobs ]","title":"get_conflict_blobs()"},{"location":"git_utils/#databooks.git_utils.get_repo","text":"Find git repo in current or parent directories. Source code in databooks/git_utils.py def get_repo ( path : Path = Path . cwd ()) -> Repo : \"\"\"Find git repo in current or parent directories.\"\"\" repo = Repo ( path = path , search_parent_directories = True ) logger . info ( f \"Repo found at: { repo . working_dir } \" ) return repo","title":"get_repo()"},{"location":"license/","text":"MIT License Copyright (c) 2021 Murilo Kuniyoshi Suzart Cunha Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"metadata/","text":"Metadata wrapper functions for cleaning notebook metadata. clear ( read_path , write_path = None , notebook_metadata_keep = (), cell_metadata_keep = (), cell_keep_fields = [], check = False , verbose = False , ** kwargs ) Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default remove all metadata. Parameters: Name Type Description Default read_path Path Path of notebook file with metadata to be cleaned required write_path Optional[pathlib.Path] Path of notebook file with metadata to be cleaned None notebook_metadata_keep Sequence[str] Notebook metadata fields to keep () cell_metadata_keep Sequence[str] Cell metadata fields to keep () cell_keep_fields List[str] Cell fields to keep [] check bool Don't write any files, check whether there is unwanted metadata False verbose bool Log written files False kwargs Any Additional keyword arguments to pass to databooks.data_models.JupyterNotebook.clear_metadata {} Returns: Type Description bool Whether notebooks are equal Source code in databooks/metadata.py def clear ( read_path : Path , write_path : Optional [ Path ] = None , notebook_metadata_keep : Sequence [ str ] = (), cell_metadata_keep : Sequence [ str ] = (), cell_keep_fields : List [ str ] = [], check : bool = False , verbose : bool = False , ** kwargs : Any , ) -> bool : \"\"\" Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default remove all metadata. :param read_path: Path of notebook file with metadata to be cleaned :param write_path: Path of notebook file with metadata to be cleaned :param notebook_metadata_keep: Notebook metadata fields to keep :param cell_metadata_keep: Cell metadata fields to keep :param cell_keep_fields: Cell fields to keep :param check: Don't write any files, check whether there is unwanted metadata :param verbose: Log written files :param kwargs: Additional keyword arguments to pass to `databooks.data_models.JupyterNotebook.clear_metadata` :return: Whether notebooks are equal \"\"\" if verbose : set_verbose ( logger ) if write_path is None : write_path = read_path notebook = JupyterNotebook . parse_file ( read_path ) # Get fields to remove from cells cell_fields = { field for cell in notebook . cells for field , _ in cell if field } cell_keep_fields += list ( Cell . __fields__ ) # required field for notebook schema cell_remove_fields = [ field for field in cell_fields if field not in cell_keep_fields ] notebook . clear_metadata ( notebook_metadata_keep = notebook_metadata_keep , cell_metadata_keep = cell_metadata_keep , cell_remove_fields = cell_remove_fields , ** kwargs , ) nb_equals = notebook == JupyterNotebook . parse_file ( read_path ) if nb_equals or check : msg = ( \"only check (unwanted metadata found).\" if not nb_equals else \"no metadata to remove.\" ) logger . debug ( f \"No action taken for { read_path } - \" + msg ) else : write_notebook ( nb = notebook , path = write_path ) logger . debug ( f \"Removed metadata from { read_path } , saved as { write_path } \" ) return nb_equals clear_all ( read_paths , write_paths , * , progress_callback =< function < lambda > at 0x7efc45485310 > , ** clear_kwargs ) Clear metadata for multiple notebooks at notebooks and cell level. Parameters: Name Type Description Default read_paths List[pathlib.Path] Paths of notebook to remove metadata required write_paths List[pathlib.Path] Paths of where to write cleaned notebooks required progress_callback Callable[[], NoneType] Callback function to report progress <function <lambda> at 0x7efc45485310> clear_kwargs Any Keyword arguments to be passed to databooks.metadata.clear {} Returns: Type Description List[bool] Whether the notebooks contained or not unwanted metadata Source code in databooks/metadata.py def clear_all ( read_paths : List [ Path ], write_paths : List [ Path ], * , progress_callback : Callable [[], None ] = lambda : None , ** clear_kwargs : Any , ) -> List [ bool ]: \"\"\" Clear metadata for multiple notebooks at notebooks and cell level. :param read_paths: Paths of notebook to remove metadata :param write_paths: Paths of where to write cleaned notebooks :param progress_callback: Callback function to report progress :param clear_kwargs: Keyword arguments to be passed to `databooks.metadata.clear` :return: Whether the notebooks contained or not unwanted metadata \"\"\" if len ( read_paths ) != len ( write_paths ): raise ValueError ( \"Read and write paths must have same length.\" f \" Got { len ( read_paths ) } and { len ( write_paths ) } \" ) checks = [] for nb_path , write_path in zip ( read_paths , write_paths ): checks . append ( clear ( read_path = nb_path , write_path = write_path , ** clear_kwargs )) progress_callback () return checks","title":"Metadata"},{"location":"metadata/#databooks.metadata.clear","text":"Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default remove all metadata. Parameters: Name Type Description Default read_path Path Path of notebook file with metadata to be cleaned required write_path Optional[pathlib.Path] Path of notebook file with metadata to be cleaned None notebook_metadata_keep Sequence[str] Notebook metadata fields to keep () cell_metadata_keep Sequence[str] Cell metadata fields to keep () cell_keep_fields List[str] Cell fields to keep [] check bool Don't write any files, check whether there is unwanted metadata False verbose bool Log written files False kwargs Any Additional keyword arguments to pass to databooks.data_models.JupyterNotebook.clear_metadata {} Returns: Type Description bool Whether notebooks are equal Source code in databooks/metadata.py def clear ( read_path : Path , write_path : Optional [ Path ] = None , notebook_metadata_keep : Sequence [ str ] = (), cell_metadata_keep : Sequence [ str ] = (), cell_keep_fields : List [ str ] = [], check : bool = False , verbose : bool = False , ** kwargs : Any , ) -> bool : \"\"\" Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default remove all metadata. :param read_path: Path of notebook file with metadata to be cleaned :param write_path: Path of notebook file with metadata to be cleaned :param notebook_metadata_keep: Notebook metadata fields to keep :param cell_metadata_keep: Cell metadata fields to keep :param cell_keep_fields: Cell fields to keep :param check: Don't write any files, check whether there is unwanted metadata :param verbose: Log written files :param kwargs: Additional keyword arguments to pass to `databooks.data_models.JupyterNotebook.clear_metadata` :return: Whether notebooks are equal \"\"\" if verbose : set_verbose ( logger ) if write_path is None : write_path = read_path notebook = JupyterNotebook . parse_file ( read_path ) # Get fields to remove from cells cell_fields = { field for cell in notebook . cells for field , _ in cell if field } cell_keep_fields += list ( Cell . __fields__ ) # required field for notebook schema cell_remove_fields = [ field for field in cell_fields if field not in cell_keep_fields ] notebook . clear_metadata ( notebook_metadata_keep = notebook_metadata_keep , cell_metadata_keep = cell_metadata_keep , cell_remove_fields = cell_remove_fields , ** kwargs , ) nb_equals = notebook == JupyterNotebook . parse_file ( read_path ) if nb_equals or check : msg = ( \"only check (unwanted metadata found).\" if not nb_equals else \"no metadata to remove.\" ) logger . debug ( f \"No action taken for { read_path } - \" + msg ) else : write_notebook ( nb = notebook , path = write_path ) logger . debug ( f \"Removed metadata from { read_path } , saved as { write_path } \" ) return nb_equals","title":"clear()"},{"location":"metadata/#databooks.metadata.clear_all","text":"Clear metadata for multiple notebooks at notebooks and cell level. Parameters: Name Type Description Default read_paths List[pathlib.Path] Paths of notebook to remove metadata required write_paths List[pathlib.Path] Paths of where to write cleaned notebooks required progress_callback Callable[[], NoneType] Callback function to report progress <function <lambda> at 0x7efc45485310> clear_kwargs Any Keyword arguments to be passed to databooks.metadata.clear {} Returns: Type Description List[bool] Whether the notebooks contained or not unwanted metadata Source code in databooks/metadata.py def clear_all ( read_paths : List [ Path ], write_paths : List [ Path ], * , progress_callback : Callable [[], None ] = lambda : None , ** clear_kwargs : Any , ) -> List [ bool ]: \"\"\" Clear metadata for multiple notebooks at notebooks and cell level. :param read_paths: Paths of notebook to remove metadata :param write_paths: Paths of where to write cleaned notebooks :param progress_callback: Callback function to report progress :param clear_kwargs: Keyword arguments to be passed to `databooks.metadata.clear` :return: Whether the notebooks contained or not unwanted metadata \"\"\" if len ( read_paths ) != len ( write_paths ): raise ValueError ( \"Read and write paths must have same length.\" f \" Got { len ( read_paths ) } and { len ( write_paths ) } \" ) checks = [] for nb_path , write_path in zip ( read_paths , write_paths ): checks . append ( clear ( read_path = nb_path , write_path = write_path , ** clear_kwargs )) progress_callback () return checks","title":"clear_all()"},{"location":"data_models/base/","text":"Data models - Base Pydantic model with custom methods. BaseCells ( UserList , Generic ) Base abstract class for notebook cells. resolve ( self , ** kwargs ) Return valid notebook cells from differences. Source code in databooks/data_models/base.py @abstractmethod def resolve ( self , ** kwargs : Any ) -> list : \"\"\"Return valid notebook cells from differences.\"\"\" raise NotImplementedError DatabooksBase ( BaseModel ) pydantic-model Base Pydantic class with extras on managing fields. Config Default configuration for base class. __str__ ( self ) special Return outputs of repr . Source code in databooks/data_models/base.py def __str__ ( self ) -> str : \"\"\"Return outputs of __repr__.\"\"\" return repr ( self ) __sub__ ( self , other ) special Subtraction between databooks.data_models.base.DatabooksBase objects. The difference basically return models that replace each fields by a tuple, where for each field we have field = (self_value, other_value) Source code in databooks/data_models/base.py def __sub__ ( self , other : DatabooksBase ) -> DiffModel : \"\"\" Subtraction between `databooks.data_models.base.DatabooksBase` objects. The difference basically return models that replace each fields by a tuple, where for each field we have `field = (self_value, other_value)` \"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # Get field and values for each instance self_d = dict ( self ) other_d = dict ( other ) # Build dict with {field: (type, value)} for each field fields_d : Dict [ str , Any ] = {} for name in self_d . keys () | other_d . keys (): self_val = self_d . get ( name ) other_val = other_d . get ( name ) if type ( self_val ) is type ( other_val ) and all ( isinstance ( val , ( DatabooksBase , BaseCells )) for val in ( self_val , other_val ) ): # Recursively get the diffs for nested models fields_d [ name ] = ( Any , self_val - other_val ) # type: ignore else : fields_d [ name ] = ( tuple , ( self_val , other_val )) # Build Pydantic models dynamically DiffInstance = create_model ( \"Diff\" + type ( self ) . __name__ , __base__ = type ( self ), resolve = resolve , is_diff = True , ** fields_d , ) return cast ( DiffModel , DiffInstance ()) # it'll be filled in with the defaults remove_fields ( self , fields , * , recursive = False , missing_ok = False ) Remove selected fields. Parameters: Name Type Description Default fields Iterable[str] Fields to remove required recursive bool Whether or not to remove the fields recursively in case of nested models False Returns: Type Description None Source code in databooks/data_models/base.py def remove_fields ( self , fields : Iterable [ str ], * , recursive : bool = False , missing_ok : bool = False , ) -> None : \"\"\" Remove selected fields. :param fields: Fields to remove :param recursive: Whether or not to remove the fields recursively in case of nested models :return: \"\"\" d_model = dict ( self ) for field in fields : field_val = d_model . get ( field ) if missing_ok else d_model [ field ] if recursive and isinstance ( field_val , DatabooksBase ): field_val . remove_fields ( fields ) elif field in d_model : delattr ( self , field ) DiffModel ( Protocol , Iterable , Generic ) Protocol for mypy static type checking. resolve ( self , * args , ** kwargs ) Return a valid base object. Source code in databooks/data_models/base.py def resolve ( self , * args : Any , ** kwargs : Any ) -> DatabooksBase : \"\"\"Return a valid base object.\"\"\" ... resolve ( model , * , keep_first = True , ignore_none = True , ** kwargs ) Resolve differences for 'diff models'. Return instance alike the parent class databooks.data_models.Cell.DatabooksBase . Parameters: Name Type Description Default model DiffModel | BaseCells DiffModel that is to be resolved (self when added as a method to a class required keep_first bool Whether to keep the information from the prior in the 'diff model' or the later True ignore_none bool Whether or not to ignore None values if encountered, and use the other field value True Returns: Type Description DatabooksBase | List[T] Model with selected fields from the differences Source code in databooks/data_models/base.py def resolve ( model : DiffModel | BaseCells , * , keep_first : bool = True , ignore_none : bool = True , ** kwargs : Any , ) -> DatabooksBase | List [ T ]: \"\"\" Resolve differences for 'diff models'. Return instance alike the parent class `databooks.data_models.Cell.DatabooksBase`. :param model: DiffModel that is to be resolved (self when added as a method to a class :param keep_first: Whether to keep the information from the prior in the 'diff model' or the later :param ignore_none: Whether or not to ignore `None` values if encountered, and use the other field value :return: Model with selected fields from the differences \"\"\" field_d = dict ( model ) is_diff = field_d . pop ( \"is_diff\" ) if not is_diff : raise TypeError ( \"Can only resolve dynamic 'diff models' (when `is_diff=True`).\" ) res_vals : Dict [ str , Any ] = {} for name , value in field_d . items (): if isinstance ( value , ( DiffModel , BaseCells )): res_vals [ name ] = value . resolve ( keep_first = keep_first , ignore_none = ignore_none , ** kwargs ) else : res_vals [ name ] = ( value [ keep_first ] if value [ not keep_first ] is None and ignore_none else value [ not keep_first ] ) return type ( model ) . mro ()[ 1 ]( ** res_vals )","title":"Base"},{"location":"data_models/base/#databooks.data_models.base.BaseCells","text":"Base abstract class for notebook cells.","title":"BaseCells"},{"location":"data_models/base/#databooks.data_models.base.BaseCells.resolve","text":"Return valid notebook cells from differences. Source code in databooks/data_models/base.py @abstractmethod def resolve ( self , ** kwargs : Any ) -> list : \"\"\"Return valid notebook cells from differences.\"\"\" raise NotImplementedError","title":"resolve()"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase","text":"Base Pydantic class with extras on managing fields.","title":"DatabooksBase"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.Config","text":"Default configuration for base class.","title":"Config"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.__str__","text":"Return outputs of repr . Source code in databooks/data_models/base.py def __str__ ( self ) -> str : \"\"\"Return outputs of __repr__.\"\"\" return repr ( self )","title":"__str__()"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.__sub__","text":"Subtraction between databooks.data_models.base.DatabooksBase objects. The difference basically return models that replace each fields by a tuple, where for each field we have field = (self_value, other_value) Source code in databooks/data_models/base.py def __sub__ ( self , other : DatabooksBase ) -> DiffModel : \"\"\" Subtraction between `databooks.data_models.base.DatabooksBase` objects. The difference basically return models that replace each fields by a tuple, where for each field we have `field = (self_value, other_value)` \"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # Get field and values for each instance self_d = dict ( self ) other_d = dict ( other ) # Build dict with {field: (type, value)} for each field fields_d : Dict [ str , Any ] = {} for name in self_d . keys () | other_d . keys (): self_val = self_d . get ( name ) other_val = other_d . get ( name ) if type ( self_val ) is type ( other_val ) and all ( isinstance ( val , ( DatabooksBase , BaseCells )) for val in ( self_val , other_val ) ): # Recursively get the diffs for nested models fields_d [ name ] = ( Any , self_val - other_val ) # type: ignore else : fields_d [ name ] = ( tuple , ( self_val , other_val )) # Build Pydantic models dynamically DiffInstance = create_model ( \"Diff\" + type ( self ) . __name__ , __base__ = type ( self ), resolve = resolve , is_diff = True , ** fields_d , ) return cast ( DiffModel , DiffInstance ()) # it'll be filled in with the defaults","title":"__sub__()"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.remove_fields","text":"Remove selected fields. Parameters: Name Type Description Default fields Iterable[str] Fields to remove required recursive bool Whether or not to remove the fields recursively in case of nested models False Returns: Type Description None Source code in databooks/data_models/base.py def remove_fields ( self , fields : Iterable [ str ], * , recursive : bool = False , missing_ok : bool = False , ) -> None : \"\"\" Remove selected fields. :param fields: Fields to remove :param recursive: Whether or not to remove the fields recursively in case of nested models :return: \"\"\" d_model = dict ( self ) for field in fields : field_val = d_model . get ( field ) if missing_ok else d_model [ field ] if recursive and isinstance ( field_val , DatabooksBase ): field_val . remove_fields ( fields ) elif field in d_model : delattr ( self , field )","title":"remove_fields()"},{"location":"data_models/base/#databooks.data_models.base.DiffModel","text":"Protocol for mypy static type checking.","title":"DiffModel"},{"location":"data_models/base/#databooks.data_models.base.DiffModel.resolve","text":"Return a valid base object. Source code in databooks/data_models/base.py def resolve ( self , * args : Any , ** kwargs : Any ) -> DatabooksBase : \"\"\"Return a valid base object.\"\"\" ...","title":"resolve()"},{"location":"data_models/base/#databooks.data_models.base.resolve","text":"Resolve differences for 'diff models'. Return instance alike the parent class databooks.data_models.Cell.DatabooksBase . Parameters: Name Type Description Default model DiffModel | BaseCells DiffModel that is to be resolved (self when added as a method to a class required keep_first bool Whether to keep the information from the prior in the 'diff model' or the later True ignore_none bool Whether or not to ignore None values if encountered, and use the other field value True Returns: Type Description DatabooksBase | List[T] Model with selected fields from the differences Source code in databooks/data_models/base.py def resolve ( model : DiffModel | BaseCells , * , keep_first : bool = True , ignore_none : bool = True , ** kwargs : Any , ) -> DatabooksBase | List [ T ]: \"\"\" Resolve differences for 'diff models'. Return instance alike the parent class `databooks.data_models.Cell.DatabooksBase`. :param model: DiffModel that is to be resolved (self when added as a method to a class :param keep_first: Whether to keep the information from the prior in the 'diff model' or the later :param ignore_none: Whether or not to ignore `None` values if encountered, and use the other field value :return: Model with selected fields from the differences \"\"\" field_d = dict ( model ) is_diff = field_d . pop ( \"is_diff\" ) if not is_diff : raise TypeError ( \"Can only resolve dynamic 'diff models' (when `is_diff=True`).\" ) res_vals : Dict [ str , Any ] = {} for name , value in field_d . items (): if isinstance ( value , ( DiffModel , BaseCells )): res_vals [ name ] = value . resolve ( keep_first = keep_first , ignore_none = ignore_none , ** kwargs ) else : res_vals [ name ] = ( value [ keep_first ] if value [ not keep_first ] is None and ignore_none else value [ not keep_first ] ) return type ( model ) . mro ()[ 1 ]( ** res_vals )","title":"resolve()"},{"location":"data_models/notebook/","text":"Data models - Jupyter Notebooks and components. Cell ( DatabooksBase ) pydantic-model Jupyter notebook cells. Fields outputs and execution_count are not included since they should only be present in code cells - thus are treated as extra fields. __hash__ ( self ) special Cells must be hashable for difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py def __hash__ ( self ) -> int : \"\"\"Cells must be hashable for `difflib.SequenceMatcher`.\"\"\" return hash ( ( type ( self ),) + tuple ( v ) if isinstance ( v , list ) else v for v in self . __dict__ . values () ) cell_has_valid_type ( v ) classmethod Check if cell has one of the three predefined types. Source code in databooks/data_models/notebook.py @validator ( \"cell_type\" ) def cell_has_valid_type ( cls , v : str ) -> str : \"\"\"Check if cell has one of the three predefined types.\"\"\" valid_cell_types = ( \"raw\" , \"markdown\" , \"code\" ) if v not in valid_cell_types : raise ValueError ( f \"Invalid cell type. Must be one of { valid_cell_types } \" ) return v clear_metadata ( self , * , cell_metadata_keep = None , cell_metadata_remove = None , cell_remove_fields = ()) Clear cell metadata, execution count and outputs. Parameters: Name Type Description Default cell_metadata_keep Sequence[str] Metadata values to keep - simply pass an empty sequence (i.e.: () ) to remove all extra fields. None cell_metadata_remove Sequence[str] Metadata values to remove None cell_remove_fields Sequence[str] Fields to remove from cell () Returns: Type Description None Source code in databooks/data_models/notebook.py def clear_metadata ( self , * , cell_metadata_keep : Sequence [ str ] = None , cell_metadata_remove : Sequence [ str ] = None , cell_remove_fields : Sequence [ str ] = (), ) -> None : \"\"\" Clear cell metadata, execution count and outputs. :param cell_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param cell_metadata_remove: Metadata values to remove :param cell_remove_fields: Fields to remove from cell :return: \"\"\" nargs = sum (( cell_metadata_keep is not None , cell_metadata_remove is not None )) if nargs != 1 : raise ValueError ( \"Exactly one of `cell_metadata_keep` or `cell_metadata_remove` must\" f \" be passed, got { nargs } arguments.\" ) if cell_metadata_keep is not None : cell_metadata_remove = tuple ( field for field , _ in self . metadata if field not in cell_metadata_keep ) self . metadata . remove_fields ( cell_metadata_remove ) # type: ignore self . remove_fields ( fields = cell_remove_fields , missing_ok = True ) must_not_be_list_for_code_cells ( values ) classmethod Check that code cells have list-type outputs. Source code in databooks/data_models/notebook.py @root_validator def must_not_be_list_for_code_cells ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Check that code cells have list-type outputs.\"\"\" if values [ \"cell_type\" ] == \"code\" and not isinstance ( values [ \"outputs\" ], list ): raise ValueError ( \"All code cells must have a list output property, got\" f \" { type ( values . get ( 'outputs' )) } \" ) return values only_code_cells_have_outputs_and_execution_count ( values ) classmethod Check that only code cells have outputs and execution count. Source code in databooks/data_models/notebook.py @root_validator def only_code_cells_have_outputs_and_execution_count ( cls , values : Dict [ str , Any ] ) -> Dict [ str , Any ]: \"\"\"Check that only code cells have outputs and execution count.\"\"\" if values [ \"cell_type\" ] != \"code\" and ( ( \"outputs\" in values ) or ( \"execution_count\" in values ) ): raise ValueError ( \"Found `outputs` or `execution_count` for cell of type\" f \" ` { values [ 'cell_type' ] } `\" ) return values remove_fields ( self , fields = (), missing_ok = True , ** kwargs ) Remove Cell fields. Similar to databooks.data_models.base.remove_fields , but include required fields for Jupyter notebook cells. Source code in databooks/data_models/notebook.py def remove_fields ( self , fields : Iterable [ str ] = (), missing_ok : bool = True , ** kwargs : Any ) -> None : \"\"\" Remove Cell fields. Similar to `databooks.data_models.base.remove_fields`, but include required fields for Jupyter notebook cells. \"\"\" # Ignore required `Cell` fields cell_fields = self . __fields__ # required fields especified in class definition if any ( field in fields for field in cell_fields ): logger . debug ( \"Ignoring removal of required fields \" + str ([ f for f in fields if f in cell_fields ]) + f \" in ` { type ( self ) . __name__ } `.\" ) fields = [ f for f in fields if f not in cell_fields ] super ( Cell , self ) . remove_fields ( fields , missing_ok = missing_ok ) if self . cell_type == \"code\" : self . outputs : List [ Dict [ str , Any ]] = ( [] if \"outputs\" not in dict ( self ) else self . outputs ) self . execution_count : Optional [ PositiveInt ] = ( None if \"execution_count\" not in dict ( self ) else self . execution_count ) CellMetadata ( DatabooksBase ) pydantic-model Cell metadata. Empty by default but can accept extra fields. Cells ( GenericModel , BaseCells ) pydantic-model Similar to list , with - operator using difflib.SequenceMatcher . data : List [ T ] property readonly Define property data required for collections.UserList class. __get_validators__ () classmethod special Get validators for custom class. Source code in databooks/data_models/notebook.py @classmethod def __get_validators__ ( cls ) -> Generator [ Callable [ ... , Any ], None , None ]: \"\"\"Get validators for custom class.\"\"\" yield cls . validate __init__ ( self , elements = ()) special Allow passing data as a positional argument when instantiating class. Source code in databooks/data_models/notebook.py def __init__ ( self , elements : Sequence [ T ] = ()) -> None : \"\"\"Allow passing data as a positional argument when instantiating class.\"\"\" super ( Cells , self ) . __init__ ( __root__ = elements ) __iter__ ( self ) special Use list property as iterable. Source code in databooks/data_models/notebook.py def __iter__ ( self ) -> Generator [ Any , None , None ]: \"\"\"Use list property as iterable.\"\"\" return ( el for el in self . data ) __sub__ ( self , other ) special Return the difference using difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py def __sub__ ( self : Cells [ Cell ], other : Cells [ Cell ] ) -> Cells [ Tuple [ List [ Cell ], List [ Cell ]]]: \"\"\"Return the difference using `difflib.SequenceMatcher`.\"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # By setting the context to the max number of cells and using # `pathlib.SequenceMatcher.get_grouped_opcodes` we essentially get the same # result as `pathlib.SequenceMatcher.get_opcodes` but in smaller chunks n_context = max ( len ( self ), len ( other )) diff_opcodes = list ( SequenceMatcher ( isjunk = None , a = self , b = other , autojunk = False ) . get_grouped_opcodes ( n_context ) ) if len ( diff_opcodes ) > 1 : raise RuntimeError ( \"Expected one group for opcodes when context size is \" f \" { n_context } for { len ( self ) } and { len ( other ) } cells in\" \" notebooks.\" ) return Cells [ Tuple [ List [ Cell ], List [ Cell ]]]( [ # https://github.com/python/mypy/issues/9459 tuple (( self . data [ i1 : j1 ], other . data [ i2 : j2 ])) # type: ignore for _ , i1 , j1 , i2 , j2 in chain . from_iterable ( diff_opcodes ) ] ) resolve ( self , * , keep_first_cells = None , first_id = None , last_id = None , ** kwargs ) Resolve differences between databooks.data_models.notebook.Cells . Parameters: Name Type Description Default keep_first_cells Optional[bool] Whether to keep the cells of the first notebook or not. If None , then keep both wrapping the git-diff tags None first_id Optional[str] Git hash of first file in conflict None last_id Optional[str] Git hash of last file in conflict None kwargs Any (Unused) keyword arguments to keep compatibility with databooks.data_models.base.resolve {} Returns: Type Description List[Cell] List of cells Source code in databooks/data_models/notebook.py def resolve ( self : Cells [ Tuple [ List [ Cell ], List [ Cell ]]], * , keep_first_cells : Optional [ bool ] = None , first_id : Optional [ str ] = None , last_id : Optional [ str ] = None , ** kwargs : Any , ) -> List [ Cell ]: \"\"\" Resolve differences between `databooks.data_models.notebook.Cells`. :param keep_first_cells: Whether to keep the cells of the first notebook or not. If `None`, then keep both wrapping the git-diff tags :param first_id: Git hash of first file in conflict :param last_id: Git hash of last file in conflict :param kwargs: (Unused) keyword arguments to keep compatibility with `databooks.data_models.base.resolve` :return: List of cells \"\"\" if keep_first_cells is not None : return list ( chain . from_iterable ( pairs [ not keep_first_cells ] for pairs in self . data ) ) return list ( chain . from_iterable ( Cells . wrap_git ( first_cells = val [ 0 ], last_cells = val [ 1 ], hash_first = first_id , hash_last = last_id , ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in self . data ) ) validate ( v ) classmethod Ensure object is custom defined container. Source code in databooks/data_models/notebook.py @classmethod def validate ( cls , v : List [ T ]) -> Cells [ T ]: \"\"\"Ensure object is custom defined container.\"\"\" if not isinstance ( v , cls ): return cls ( v ) else : return v wrap_git ( first_cells , last_cells , hash_first = None , hash_last = None ) classmethod Wrap git-diff cells in existing notebook. Source code in databooks/data_models/notebook.py @classmethod def wrap_git ( cls , first_cells : List [ Cell ], last_cells : List [ Cell ], hash_first : Optional [ str ] = None , hash_last : Optional [ str ] = None , ) -> List [ Cell ]: \"\"\"Wrap git-diff cells in existing notebook.\"\"\" return ( [ Cell ( metadata = CellMetadata ( git_hash = hash_first ), source = [ f \"`<<<<<<< { hash_first } `\" ], cell_type = \"markdown\" , ) ] + first_cells + [ Cell ( source = [ \"`=======`\" ], cell_type = \"markdown\" , metadata = CellMetadata (), ) ] + last_cells + [ Cell ( metadata = CellMetadata ( git_hash = hash_last ), source = [ f \"`>>>>>>> { hash_last } `\" ], cell_type = \"markdown\" , ) ] ) Cells[Cell] ( Cells ) pydantic-model Config getter_dict ( Representation ) Hack to make object's smell just enough like dicts for validate_model. We can't inherit from Mapping[str, Any] because it upsets cython so we have to implement all methods ourselves. get_field_info ( name ) classmethod Get properties of FieldInfo from the fields property of the config class. json_dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ) Serialize obj to a JSON formatted str . If skipkeys is true then dict keys that are not basic types ( str , int , float , bool , None ) will be skipped instead of raising a TypeError . If ensure_ascii is false, then the return value can contain non-ASCII characters if they appear in strings contained in obj . Otherwise, all such characters are escaped in JSON strings. If check_circular is false, then the circular reference check for container types will be skipped and a circular reference will result in an OverflowError (or worse). If allow_nan is false, then it will be a ValueError to serialize out of range float values ( nan , inf , -inf ) in strict compliance of the JSON specification, instead of using the JavaScript equivalents ( NaN , Infinity , -Infinity ). If indent is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. If specified, separators should be an (item_separator, key_separator) tuple. The default is (', ', ': ') if indent is None and (',', ': ') otherwise. To get the most compact JSON representation, you should specify (',', ':') to eliminate whitespace. default(obj) is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If sort_keys is true (default: False ), then the output of dictionaries will be sorted by key. To use a custom JSONEncoder subclass (e.g. one that overrides the .default() method to serialize additional types), specify it with the cls kwarg; otherwise JSONEncoder is used. Source code in databooks/data_models/notebook.py def dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ): \"\"\"Serialize ``obj`` to a JSON formatted ``str``. If ``skipkeys`` is true then ``dict`` keys that are not basic types (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped instead of raising a ``TypeError``. If ``ensure_ascii`` is false, then the return value can contain non-ASCII characters if they appear in strings contained in ``obj``. Otherwise, all such characters are escaped in JSON strings. If ``check_circular`` is false, then the circular reference check for container types will be skipped and a circular reference will result in an ``OverflowError`` (or worse). If ``allow_nan`` is false, then it will be a ``ValueError`` to serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in strict compliance of the JSON specification, instead of using the JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). If ``indent`` is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact representation. If specified, ``separators`` should be an ``(item_separator, key_separator)`` tuple. The default is ``(', ', ': ')`` if *indent* is ``None`` and ``(',', ': ')`` otherwise. To get the most compact JSON representation, you should specify ``(',', ':')`` to eliminate whitespace. ``default(obj)`` is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If *sort_keys* is true (default: ``False``), then the output of dictionaries will be sorted by key. To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the ``.default()`` method to serialize additional types), specify it with the ``cls`` kwarg; otherwise ``JSONEncoder`` is used. \"\"\" # cached encoder if ( not skipkeys and ensure_ascii and check_circular and allow_nan and cls is None and indent is None and separators is None and default is None and not sort_keys and not kw ): return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) . encode ( obj ) json_loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ) Deserialize s (a str , bytes or bytearray instance containing a JSON document) to a Python object. object_hook is an optional function that will be called with the result of any object literal decode (a dict ). The return value of object_hook will be used instead of the dict . This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). object_pairs_hook is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of object_pairs_hook will be used instead of the dict . This feature can be used to implement custom decoders. If object_hook is also defined, the object_pairs_hook takes priority. parse_float , if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). parse_int , if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). parse_constant , if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom JSONDecoder subclass, specify it with the cls kwarg; otherwise JSONDecoder is used. The encoding argument is ignored and deprecated since Python 3.1. Source code in databooks/data_models/notebook.py def loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ): \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance containing a JSON document) to a Python object. ``object_hook`` is an optional function that will be called with the result of any object literal decode (a ``dict``). The return value of ``object_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). ``object_pairs_hook`` is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of ``object_pairs_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders. If ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority. ``parse_float``, if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). ``parse_int``, if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). ``parse_constant``, if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom ``JSONDecoder`` subclass, specify it with the ``cls`` kwarg; otherwise ``JSONDecoder`` is used. The ``encoding`` argument is ignored and deprecated since Python 3.1. \"\"\" if isinstance ( s , str ): if s . startswith ( ' \\ufeff ' ): raise JSONDecodeError ( \"Unexpected UTF-8 BOM (decode using utf-8-sig)\" , s , 0 ) else : if not isinstance ( s , ( bytes , bytearray )): raise TypeError ( f 'the JSON object must be str, bytes or bytearray, ' f 'not { s . __class__ . __name__ } ' ) s = s . decode ( detect_encoding ( s ), 'surrogatepass' ) if \"encoding\" in kw : import warnings warnings . warn ( \"'encoding' is ignored and deprecated. It will be removed in Python 3.9\" , DeprecationWarning , stacklevel = 2 ) del kw [ 'encoding' ] if ( cls is None and object_hook is None and parse_int is None and parse_float is None and parse_constant is None and object_pairs_hook is None and not kw ): return _default_decoder . decode ( s ) if cls is None : cls = JSONDecoder if object_hook is not None : kw [ 'object_hook' ] = object_hook if object_pairs_hook is not None : kw [ 'object_pairs_hook' ] = object_pairs_hook if parse_float is not None : kw [ 'parse_float' ] = parse_float if parse_int is not None : kw [ 'parse_int' ] = parse_int if parse_constant is not None : kw [ 'parse_constant' ] = parse_constant return cls ( ** kw ) . decode ( s ) prepare_field ( field ) classmethod Optional hook to check or modify fields during model creation. JupyterNotebook ( DatabooksBase ) pydantic-model Jupyter notebook. Extra fields yield invalid notebook. clear_metadata ( self , * , notebook_metadata_keep = None , notebook_metadata_remove = None , ** cell_kwargs ) Clear notebook and cell metadata. Parameters: Name Type Description Default notebook_metadata_keep Sequence[str] Metadata values to keep - simply pass an empty sequence (i.e.: () ) to remove all extra fields. None notebook_metadata_remove Sequence[str] Metadata values to remove None cell_kwargs Any keyword arguments to be passed to each cell's databooks.data_models.Cell.clear_metadata {} Returns: Type Description None Source code in databooks/data_models/notebook.py def clear_metadata ( self , * , notebook_metadata_keep : Sequence [ str ] = None , notebook_metadata_remove : Sequence [ str ] = None , ** cell_kwargs : Any , ) -> None : \"\"\" Clear notebook and cell metadata. :param notebook_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param notebook_metadata_remove: Metadata values to remove :param cell_kwargs: keyword arguments to be passed to each cell's `databooks.data_models.Cell.clear_metadata` :return: \"\"\" nargs = sum ( ( notebook_metadata_keep is not None , notebook_metadata_remove is not None ) ) if nargs != 1 : raise ValueError ( \"Exactly one of `notebook_metadata_keep` or `notebook_metadata_remove`\" f \" must be passed, got { nargs } arguments.\" ) if notebook_metadata_keep is not None : notebook_metadata_remove = tuple ( field for field , _ in self . metadata if field not in notebook_metadata_keep ) self . metadata . remove_fields ( notebook_metadata_remove ) # type: ignore if len ( cell_kwargs ) > 0 : _clean_cells = deepcopy ( self . cells ) for cell in _clean_cells : cell . clear_metadata ( ** cell_kwargs ) self . cells = _clean_cells parse_file ( path , ** parse_kwargs ) classmethod Parse notebook from a path. Source code in databooks/data_models/notebook.py @classmethod def parse_file ( cls , path : Path | str , ** parse_kwargs : Any ) -> JupyterNotebook : \"\"\"Parse notebook from a path.\"\"\" content_arg = parse_kwargs . pop ( \"content_type\" , None ) if content_arg is not None : raise ValueError ( f \"Value of `content_type` must be `json` (default), got ` { content_arg } `\" ) return super ( JupyterNotebook , cls ) . parse_file ( path = path , content_type = \"json\" , ** parse_kwargs ) NotebookMetadata ( DatabooksBase ) pydantic-model Notebook metadata. Empty by default but can accept extra fields.","title":"Notebooks"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cell","text":"Jupyter notebook cells. Fields outputs and execution_count are not included since they should only be present in code cells - thus are treated as extra fields.","title":"Cell"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cell.__hash__","text":"Cells must be hashable for difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py def __hash__ ( self ) -> int : \"\"\"Cells must be hashable for `difflib.SequenceMatcher`.\"\"\" return hash ( ( type ( self ),) + tuple ( v ) if isinstance ( v , list ) else v for v in self . __dict__ . values () )","title":"__hash__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cell.cell_has_valid_type","text":"Check if cell has one of the three predefined types. Source code in databooks/data_models/notebook.py @validator ( \"cell_type\" ) def cell_has_valid_type ( cls , v : str ) -> str : \"\"\"Check if cell has one of the three predefined types.\"\"\" valid_cell_types = ( \"raw\" , \"markdown\" , \"code\" ) if v not in valid_cell_types : raise ValueError ( f \"Invalid cell type. Must be one of { valid_cell_types } \" ) return v","title":"cell_has_valid_type()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cell.clear_metadata","text":"Clear cell metadata, execution count and outputs. Parameters: Name Type Description Default cell_metadata_keep Sequence[str] Metadata values to keep - simply pass an empty sequence (i.e.: () ) to remove all extra fields. None cell_metadata_remove Sequence[str] Metadata values to remove None cell_remove_fields Sequence[str] Fields to remove from cell () Returns: Type Description None Source code in databooks/data_models/notebook.py def clear_metadata ( self , * , cell_metadata_keep : Sequence [ str ] = None , cell_metadata_remove : Sequence [ str ] = None , cell_remove_fields : Sequence [ str ] = (), ) -> None : \"\"\" Clear cell metadata, execution count and outputs. :param cell_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param cell_metadata_remove: Metadata values to remove :param cell_remove_fields: Fields to remove from cell :return: \"\"\" nargs = sum (( cell_metadata_keep is not None , cell_metadata_remove is not None )) if nargs != 1 : raise ValueError ( \"Exactly one of `cell_metadata_keep` or `cell_metadata_remove` must\" f \" be passed, got { nargs } arguments.\" ) if cell_metadata_keep is not None : cell_metadata_remove = tuple ( field for field , _ in self . metadata if field not in cell_metadata_keep ) self . metadata . remove_fields ( cell_metadata_remove ) # type: ignore self . remove_fields ( fields = cell_remove_fields , missing_ok = True )","title":"clear_metadata()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cell.must_not_be_list_for_code_cells","text":"Check that code cells have list-type outputs. Source code in databooks/data_models/notebook.py @root_validator def must_not_be_list_for_code_cells ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Check that code cells have list-type outputs.\"\"\" if values [ \"cell_type\" ] == \"code\" and not isinstance ( values [ \"outputs\" ], list ): raise ValueError ( \"All code cells must have a list output property, got\" f \" { type ( values . get ( 'outputs' )) } \" ) return values","title":"must_not_be_list_for_code_cells()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cell.only_code_cells_have_outputs_and_execution_count","text":"Check that only code cells have outputs and execution count. Source code in databooks/data_models/notebook.py @root_validator def only_code_cells_have_outputs_and_execution_count ( cls , values : Dict [ str , Any ] ) -> Dict [ str , Any ]: \"\"\"Check that only code cells have outputs and execution count.\"\"\" if values [ \"cell_type\" ] != \"code\" and ( ( \"outputs\" in values ) or ( \"execution_count\" in values ) ): raise ValueError ( \"Found `outputs` or `execution_count` for cell of type\" f \" ` { values [ 'cell_type' ] } `\" ) return values","title":"only_code_cells_have_outputs_and_execution_count()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cell.remove_fields","text":"Remove Cell fields. Similar to databooks.data_models.base.remove_fields , but include required fields for Jupyter notebook cells. Source code in databooks/data_models/notebook.py def remove_fields ( self , fields : Iterable [ str ] = (), missing_ok : bool = True , ** kwargs : Any ) -> None : \"\"\" Remove Cell fields. Similar to `databooks.data_models.base.remove_fields`, but include required fields for Jupyter notebook cells. \"\"\" # Ignore required `Cell` fields cell_fields = self . __fields__ # required fields especified in class definition if any ( field in fields for field in cell_fields ): logger . debug ( \"Ignoring removal of required fields \" + str ([ f for f in fields if f in cell_fields ]) + f \" in ` { type ( self ) . __name__ } `.\" ) fields = [ f for f in fields if f not in cell_fields ] super ( Cell , self ) . remove_fields ( fields , missing_ok = missing_ok ) if self . cell_type == \"code\" : self . outputs : List [ Dict [ str , Any ]] = ( [] if \"outputs\" not in dict ( self ) else self . outputs ) self . execution_count : Optional [ PositiveInt ] = ( None if \"execution_count\" not in dict ( self ) else self . execution_count )","title":"remove_fields()"},{"location":"data_models/notebook/#databooks.data_models.notebook.CellMetadata","text":"Cell metadata. Empty by default but can accept extra fields.","title":"CellMetadata"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells","text":"Similar to list , with - operator using difflib.SequenceMatcher .","title":"Cells"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.data","text":"Define property data required for collections.UserList class.","title":"data"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__get_validators__","text":"Get validators for custom class. Source code in databooks/data_models/notebook.py @classmethod def __get_validators__ ( cls ) -> Generator [ Callable [ ... , Any ], None , None ]: \"\"\"Get validators for custom class.\"\"\" yield cls . validate","title":"__get_validators__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__init__","text":"Allow passing data as a positional argument when instantiating class. Source code in databooks/data_models/notebook.py def __init__ ( self , elements : Sequence [ T ] = ()) -> None : \"\"\"Allow passing data as a positional argument when instantiating class.\"\"\" super ( Cells , self ) . __init__ ( __root__ = elements )","title":"__init__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__iter__","text":"Use list property as iterable. Source code in databooks/data_models/notebook.py def __iter__ ( self ) -> Generator [ Any , None , None ]: \"\"\"Use list property as iterable.\"\"\" return ( el for el in self . data )","title":"__iter__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__sub__","text":"Return the difference using difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py def __sub__ ( self : Cells [ Cell ], other : Cells [ Cell ] ) -> Cells [ Tuple [ List [ Cell ], List [ Cell ]]]: \"\"\"Return the difference using `difflib.SequenceMatcher`.\"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # By setting the context to the max number of cells and using # `pathlib.SequenceMatcher.get_grouped_opcodes` we essentially get the same # result as `pathlib.SequenceMatcher.get_opcodes` but in smaller chunks n_context = max ( len ( self ), len ( other )) diff_opcodes = list ( SequenceMatcher ( isjunk = None , a = self , b = other , autojunk = False ) . get_grouped_opcodes ( n_context ) ) if len ( diff_opcodes ) > 1 : raise RuntimeError ( \"Expected one group for opcodes when context size is \" f \" { n_context } for { len ( self ) } and { len ( other ) } cells in\" \" notebooks.\" ) return Cells [ Tuple [ List [ Cell ], List [ Cell ]]]( [ # https://github.com/python/mypy/issues/9459 tuple (( self . data [ i1 : j1 ], other . data [ i2 : j2 ])) # type: ignore for _ , i1 , j1 , i2 , j2 in chain . from_iterable ( diff_opcodes ) ] )","title":"__sub__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.resolve","text":"Resolve differences between databooks.data_models.notebook.Cells . Parameters: Name Type Description Default keep_first_cells Optional[bool] Whether to keep the cells of the first notebook or not. If None , then keep both wrapping the git-diff tags None first_id Optional[str] Git hash of first file in conflict None last_id Optional[str] Git hash of last file in conflict None kwargs Any (Unused) keyword arguments to keep compatibility with databooks.data_models.base.resolve {} Returns: Type Description List[Cell] List of cells Source code in databooks/data_models/notebook.py def resolve ( self : Cells [ Tuple [ List [ Cell ], List [ Cell ]]], * , keep_first_cells : Optional [ bool ] = None , first_id : Optional [ str ] = None , last_id : Optional [ str ] = None , ** kwargs : Any , ) -> List [ Cell ]: \"\"\" Resolve differences between `databooks.data_models.notebook.Cells`. :param keep_first_cells: Whether to keep the cells of the first notebook or not. If `None`, then keep both wrapping the git-diff tags :param first_id: Git hash of first file in conflict :param last_id: Git hash of last file in conflict :param kwargs: (Unused) keyword arguments to keep compatibility with `databooks.data_models.base.resolve` :return: List of cells \"\"\" if keep_first_cells is not None : return list ( chain . from_iterable ( pairs [ not keep_first_cells ] for pairs in self . data ) ) return list ( chain . from_iterable ( Cells . wrap_git ( first_cells = val [ 0 ], last_cells = val [ 1 ], hash_first = first_id , hash_last = last_id , ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in self . data ) )","title":"resolve()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.validate","text":"Ensure object is custom defined container. Source code in databooks/data_models/notebook.py @classmethod def validate ( cls , v : List [ T ]) -> Cells [ T ]: \"\"\"Ensure object is custom defined container.\"\"\" if not isinstance ( v , cls ): return cls ( v ) else : return v","title":"validate()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.wrap_git","text":"Wrap git-diff cells in existing notebook. Source code in databooks/data_models/notebook.py @classmethod def wrap_git ( cls , first_cells : List [ Cell ], last_cells : List [ Cell ], hash_first : Optional [ str ] = None , hash_last : Optional [ str ] = None , ) -> List [ Cell ]: \"\"\"Wrap git-diff cells in existing notebook.\"\"\" return ( [ Cell ( metadata = CellMetadata ( git_hash = hash_first ), source = [ f \"`<<<<<<< { hash_first } `\" ], cell_type = \"markdown\" , ) ] + first_cells + [ Cell ( source = [ \"`=======`\" ], cell_type = \"markdown\" , metadata = CellMetadata (), ) ] + last_cells + [ Cell ( metadata = CellMetadata ( git_hash = hash_last ), source = [ f \"`>>>>>>> { hash_last } `\" ], cell_type = \"markdown\" , ) ] )","title":"wrap_git()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Cell]","text":"","title":"Cells[Cell]"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Cell].Config","text":"","title":"Config"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Cell].Config.getter_dict","text":"Hack to make object's smell just enough like dicts for validate_model. We can't inherit from Mapping[str, Any] because it upsets cython so we have to implement all methods ourselves.","title":"getter_dict"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Cell].Config.get_field_info","text":"Get properties of FieldInfo from the fields property of the config class.","title":"get_field_info()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Cell].Config.json_dumps","text":"Serialize obj to a JSON formatted str . If skipkeys is true then dict keys that are not basic types ( str , int , float , bool , None ) will be skipped instead of raising a TypeError . If ensure_ascii is false, then the return value can contain non-ASCII characters if they appear in strings contained in obj . Otherwise, all such characters are escaped in JSON strings. If check_circular is false, then the circular reference check for container types will be skipped and a circular reference will result in an OverflowError (or worse). If allow_nan is false, then it will be a ValueError to serialize out of range float values ( nan , inf , -inf ) in strict compliance of the JSON specification, instead of using the JavaScript equivalents ( NaN , Infinity , -Infinity ). If indent is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. If specified, separators should be an (item_separator, key_separator) tuple. The default is (', ', ': ') if indent is None and (',', ': ') otherwise. To get the most compact JSON representation, you should specify (',', ':') to eliminate whitespace. default(obj) is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If sort_keys is true (default: False ), then the output of dictionaries will be sorted by key. To use a custom JSONEncoder subclass (e.g. one that overrides the .default() method to serialize additional types), specify it with the cls kwarg; otherwise JSONEncoder is used. Source code in databooks/data_models/notebook.py def dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ): \"\"\"Serialize ``obj`` to a JSON formatted ``str``. If ``skipkeys`` is true then ``dict`` keys that are not basic types (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped instead of raising a ``TypeError``. If ``ensure_ascii`` is false, then the return value can contain non-ASCII characters if they appear in strings contained in ``obj``. Otherwise, all such characters are escaped in JSON strings. If ``check_circular`` is false, then the circular reference check for container types will be skipped and a circular reference will result in an ``OverflowError`` (or worse). If ``allow_nan`` is false, then it will be a ``ValueError`` to serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in strict compliance of the JSON specification, instead of using the JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). If ``indent`` is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact representation. If specified, ``separators`` should be an ``(item_separator, key_separator)`` tuple. The default is ``(', ', ': ')`` if *indent* is ``None`` and ``(',', ': ')`` otherwise. To get the most compact JSON representation, you should specify ``(',', ':')`` to eliminate whitespace. ``default(obj)`` is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If *sort_keys* is true (default: ``False``), then the output of dictionaries will be sorted by key. To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the ``.default()`` method to serialize additional types), specify it with the ``cls`` kwarg; otherwise ``JSONEncoder`` is used. \"\"\" # cached encoder if ( not skipkeys and ensure_ascii and check_circular and allow_nan and cls is None and indent is None and separators is None and default is None and not sort_keys and not kw ): return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) . encode ( obj )","title":"json_dumps()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Cell].Config.json_loads","text":"Deserialize s (a str , bytes or bytearray instance containing a JSON document) to a Python object. object_hook is an optional function that will be called with the result of any object literal decode (a dict ). The return value of object_hook will be used instead of the dict . This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). object_pairs_hook is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of object_pairs_hook will be used instead of the dict . This feature can be used to implement custom decoders. If object_hook is also defined, the object_pairs_hook takes priority. parse_float , if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). parse_int , if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). parse_constant , if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom JSONDecoder subclass, specify it with the cls kwarg; otherwise JSONDecoder is used. The encoding argument is ignored and deprecated since Python 3.1. Source code in databooks/data_models/notebook.py def loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ): \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance containing a JSON document) to a Python object. ``object_hook`` is an optional function that will be called with the result of any object literal decode (a ``dict``). The return value of ``object_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). ``object_pairs_hook`` is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of ``object_pairs_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders. If ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority. ``parse_float``, if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). ``parse_int``, if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). ``parse_constant``, if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom ``JSONDecoder`` subclass, specify it with the ``cls`` kwarg; otherwise ``JSONDecoder`` is used. The ``encoding`` argument is ignored and deprecated since Python 3.1. \"\"\" if isinstance ( s , str ): if s . startswith ( ' \\ufeff ' ): raise JSONDecodeError ( \"Unexpected UTF-8 BOM (decode using utf-8-sig)\" , s , 0 ) else : if not isinstance ( s , ( bytes , bytearray )): raise TypeError ( f 'the JSON object must be str, bytes or bytearray, ' f 'not { s . __class__ . __name__ } ' ) s = s . decode ( detect_encoding ( s ), 'surrogatepass' ) if \"encoding\" in kw : import warnings warnings . warn ( \"'encoding' is ignored and deprecated. It will be removed in Python 3.9\" , DeprecationWarning , stacklevel = 2 ) del kw [ 'encoding' ] if ( cls is None and object_hook is None and parse_int is None and parse_float is None and parse_constant is None and object_pairs_hook is None and not kw ): return _default_decoder . decode ( s ) if cls is None : cls = JSONDecoder if object_hook is not None : kw [ 'object_hook' ] = object_hook if object_pairs_hook is not None : kw [ 'object_pairs_hook' ] = object_pairs_hook if parse_float is not None : kw [ 'parse_float' ] = parse_float if parse_int is not None : kw [ 'parse_int' ] = parse_int if parse_constant is not None : kw [ 'parse_constant' ] = parse_constant return cls ( ** kw ) . decode ( s )","title":"json_loads()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Cell].Config.prepare_field","text":"Optional hook to check or modify fields during model creation.","title":"prepare_field()"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook","text":"Jupyter notebook. Extra fields yield invalid notebook.","title":"JupyterNotebook"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook.clear_metadata","text":"Clear notebook and cell metadata. Parameters: Name Type Description Default notebook_metadata_keep Sequence[str] Metadata values to keep - simply pass an empty sequence (i.e.: () ) to remove all extra fields. None notebook_metadata_remove Sequence[str] Metadata values to remove None cell_kwargs Any keyword arguments to be passed to each cell's databooks.data_models.Cell.clear_metadata {} Returns: Type Description None Source code in databooks/data_models/notebook.py def clear_metadata ( self , * , notebook_metadata_keep : Sequence [ str ] = None , notebook_metadata_remove : Sequence [ str ] = None , ** cell_kwargs : Any , ) -> None : \"\"\" Clear notebook and cell metadata. :param notebook_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param notebook_metadata_remove: Metadata values to remove :param cell_kwargs: keyword arguments to be passed to each cell's `databooks.data_models.Cell.clear_metadata` :return: \"\"\" nargs = sum ( ( notebook_metadata_keep is not None , notebook_metadata_remove is not None ) ) if nargs != 1 : raise ValueError ( \"Exactly one of `notebook_metadata_keep` or `notebook_metadata_remove`\" f \" must be passed, got { nargs } arguments.\" ) if notebook_metadata_keep is not None : notebook_metadata_remove = tuple ( field for field , _ in self . metadata if field not in notebook_metadata_keep ) self . metadata . remove_fields ( notebook_metadata_remove ) # type: ignore if len ( cell_kwargs ) > 0 : _clean_cells = deepcopy ( self . cells ) for cell in _clean_cells : cell . clear_metadata ( ** cell_kwargs ) self . cells = _clean_cells","title":"clear_metadata()"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook.parse_file","text":"Parse notebook from a path. Source code in databooks/data_models/notebook.py @classmethod def parse_file ( cls , path : Path | str , ** parse_kwargs : Any ) -> JupyterNotebook : \"\"\"Parse notebook from a path.\"\"\" content_arg = parse_kwargs . pop ( \"content_type\" , None ) if content_arg is not None : raise ValueError ( f \"Value of `content_type` must be `json` (default), got ` { content_arg } `\" ) return super ( JupyterNotebook , cls ) . parse_file ( path = path , content_type = \"json\" , ** parse_kwargs )","title":"parse_file()"},{"location":"data_models/notebook/#databooks.data_models.notebook.NotebookMetadata","text":"Notebook metadata. Empty by default but can accept extra fields.","title":"NotebookMetadata"},{"location":"usage/cicd/","text":"CI/CD CI/CD essentially runs some code in a remote server, triggered by git events. In these runs, one could trigger other events or simply check whether the code is up to standards. This is a nice way to make sure that all the code that is in your git repo passes all quality checks. GitHub Actions GitHub Actions are a GitHub-hosted solution for CI/CD. All you need to get started is a file in project_root/.github/workflows/nb-meta.yml . An example workflow to clean any notebook metadata and commit changes at every push: name: 'nb-meta' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Configure git user run: | git config --global user.name 'github-actions[bot]' git config --global user.email 'github-actions[bot]@users.noreply.github.com' - name: Install dependencies and clean metadata run: | pip install databooks databooks meta . --overwrite - name: Commit changes and push run: | git commit -am \"Automated commit - clean notebook metadata\" git push Alternatively, one can choose to avoid having CI systems making code changes. In that case, we can only check whether notebooks have any undesired metadata. name: 'nb-meta' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Install dependencies and check metadata run: | pip install databooks databooks meta . --check","title":"CI/CD"},{"location":"usage/cicd/#cicd","text":"CI/CD essentially runs some code in a remote server, triggered by git events. In these runs, one could trigger other events or simply check whether the code is up to standards. This is a nice way to make sure that all the code that is in your git repo passes all quality checks.","title":"CI/CD"},{"location":"usage/cicd/#github-actions","text":"GitHub Actions are a GitHub-hosted solution for CI/CD. All you need to get started is a file in project_root/.github/workflows/nb-meta.yml . An example workflow to clean any notebook metadata and commit changes at every push: name: 'nb-meta' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Configure git user run: | git config --global user.name 'github-actions[bot]' git config --global user.email 'github-actions[bot]@users.noreply.github.com' - name: Install dependencies and clean metadata run: | pip install databooks databooks meta . --overwrite - name: Commit changes and push run: | git commit -am \"Automated commit - clean notebook metadata\" git push Alternatively, one can choose to avoid having CI systems making code changes. In that case, we can only check whether notebooks have any undesired metadata. name: 'nb-meta' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Install dependencies and check metadata run: | pip install databooks databooks meta . --check","title":"GitHub Actions"},{"location":"usage/cli/","text":"CLI tool The most straightforward way is to use it in the terminal, whenever desired. That can be error-prone and have \"dirty\" notebooks in your git repo. Check CLI documentation for more information. A safer alternative is to automate this step, by setting up CI in your repo or a pre-commit hook.","title":"CLI"},{"location":"usage/cli/#cli-tool","text":"The most straightforward way is to use it in the terminal, whenever desired. That can be error-prone and have \"dirty\" notebooks in your git repo. Check CLI documentation for more information. A safer alternative is to automate this step, by setting up CI in your repo or a pre-commit hook.","title":"CLI tool"},{"location":"usage/overview/","text":"Usage databooks is a CLI tool, but there are many ways in which one can make use of the tool - such as CI or hooks.","title":"Overview"},{"location":"usage/overview/#usage","text":"databooks is a CLI tool, but there are many ways in which one can make use of the tool - such as CI or hooks.","title":"Usage"},{"location":"usage/precommit/","text":"Pre-commit hooks Another alternative is to try to catch code quality issues before any code is even sent to the remote git repo. Pre-commit hooks are essentially actions that are taken right before code is committed to your (local) repo. Pre-commit package There are different ways to create new hooks to your git repo. pre-commit is a package to easily config pre-commit hooks, and store them in a very readable manner. Installation To install, simply run: pip install pre-commit Usage Configuration To use pre-commit , create a .pre-commit-config.yaml in the root of your project. There, include repos: - repo: https://github.com/datarootsio/databooks rev: 0.1.3 hooks: - id: databooks args: [\"--overwrite\"] databooks repo has minimal configuration (such as the meta command). The rev parameter indicates the version to use and args indicate additional arguments to pass to the tool. In the example above, we opt to overwrite the files. The pre-commit tool doesn't actually commit any changes if the staged files are modified. Therefore, if there is any unwanted metadata at the time of committing the changes, the files would be modified, no commit would be made, and it'd be up to the developer to inspect the changes, add them and commit. That's why we recommend specifying args: [\"--overwrite\"] . You can require the developer to manually remove metadata, by specifying args: [\"--check\"] . In this case the commit would still be aborted, but no files would be changed. The .pre-commit-config.yaml would look like repos: - repo: https://github.com/datarootsio/databooks rev: 0.1.3 hooks: - id: databooks args: [\"--check\"] Running Once the configuration is in place all the user needs to do to trigger pre-commit is to commit changes normally $ git add path/to/notebook.ipynb $ git commit -m 'a clear message' databooks................................................................Failed - hook id: databooks - files were modified by this hook [20:51:08] WARNING 1 files will be overwritten cli.py:81 Removing metadata \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 INFO The metadata of 1 out of 1 notebooks were cli.py:114 removed! Alternatively, one could run pre-commit run to manually run the same command that is triggered right before committing changes. Or, one could run pre-commit run --all-files to run the pre-commit hooks in all files (regardless if the files have been staged or not). The later is useful as a first-run to ensure consistency across the git repo or in CI.","title":"Pre-commit hooks"},{"location":"usage/precommit/#pre-commit-hooks","text":"Another alternative is to try to catch code quality issues before any code is even sent to the remote git repo. Pre-commit hooks are essentially actions that are taken right before code is committed to your (local) repo.","title":"Pre-commit hooks"},{"location":"usage/precommit/#pre-commit-package","text":"There are different ways to create new hooks to your git repo. pre-commit is a package to easily config pre-commit hooks, and store them in a very readable manner.","title":"Pre-commit package"},{"location":"usage/precommit/#installation","text":"To install, simply run: pip install pre-commit","title":"Installation"},{"location":"usage/precommit/#usage","text":"","title":"Usage"},{"location":"usage/precommit/#configuration","text":"To use pre-commit , create a .pre-commit-config.yaml in the root of your project. There, include repos: - repo: https://github.com/datarootsio/databooks rev: 0.1.3 hooks: - id: databooks args: [\"--overwrite\"] databooks repo has minimal configuration (such as the meta command). The rev parameter indicates the version to use and args indicate additional arguments to pass to the tool. In the example above, we opt to overwrite the files. The pre-commit tool doesn't actually commit any changes if the staged files are modified. Therefore, if there is any unwanted metadata at the time of committing the changes, the files would be modified, no commit would be made, and it'd be up to the developer to inspect the changes, add them and commit. That's why we recommend specifying args: [\"--overwrite\"] . You can require the developer to manually remove metadata, by specifying args: [\"--check\"] . In this case the commit would still be aborted, but no files would be changed. The .pre-commit-config.yaml would look like repos: - repo: https://github.com/datarootsio/databooks rev: 0.1.3 hooks: - id: databooks args: [\"--check\"]","title":"Configuration"},{"location":"usage/precommit/#running","text":"Once the configuration is in place all the user needs to do to trigger pre-commit is to commit changes normally $ git add path/to/notebook.ipynb $ git commit -m 'a clear message' databooks................................................................Failed - hook id: databooks - files were modified by this hook [20:51:08] WARNING 1 files will be overwritten cli.py:81 Removing metadata \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 INFO The metadata of 1 out of 1 notebooks were cli.py:114 removed! Alternatively, one could run pre-commit run to manually run the same command that is triggered right before committing changes. Or, one could run pre-commit run --all-files to run the pre-commit hooks in all files (regardless if the files have been staged or not). The later is useful as a first-run to ensure consistency across the git repo or in CI.","title":"Running"},{"location":"coverage/","text":".md-content { max-width: none !important; } article h1, article > a { display: none; } var coviframe = document.getElementById(\"coviframe\"); function resizeIframe() { coviframe.style.height = coviframe.contentWindow.document.documentElement.offsetHeight + 'px'; } coviframe.contentWindow.document.body.onclick = function() { coviframe.contentWindow.location.reload(); }","title":"Coverage report"}]}